{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random as rd\n",
    "from unidecode import unidecode\n",
    "import nltk as nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.corpus import floresta\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'anotados_3929.json'\n",
    "with open('C:\\\\Users\\\\92007848\\\\Documents\\\\AIProj\\\\AiTeste\\\\NLTKAlpha\\\\Resources\\\\'+file,encoding='Utf-8') as json_file:\n",
    "    conteudo = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rd.shuffle(conteudo)\n",
    "linha = 0\n",
    "anotacao = 0\n",
    "##Analise do dataset\n",
    "print(conteudo[linha])\n",
    "print('')\n",
    "print(conteudo[linha]['content'])\n",
    "print('')\n",
    "print(conteudo[linha]['intent'])\n",
    "print('')\n",
    "print('Anotações')\n",
    "print(conteudo[linha]['annotation'])\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Criar um ID para as entradas do dataSet\n",
    "conteudo_id = []\n",
    "for line , data in enumerate(conteudo):\n",
    "    element = {line: data}\n",
    "    conteudo_id.append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "linha = 21\n",
    "anotacao = 0\n",
    "id = 0\n",
    "##Analise do dataset\n",
    "print(conteudo_id[linha])\n",
    "print('')\n",
    "print(conteudo_id[linha][id]['content'])\n",
    "print('')\n",
    "print(conteudo_id[linha][id]['intent'])\n",
    "print('')\n",
    "print('Anotações')\n",
    "print(conteudo_id[linha][id]['annotation'][anotacao])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando os conteudos de cada trecho\n",
    "#Separando o content, annotation e as keywords\n",
    "contents = {}\n",
    "annotations = {}\n",
    "keywords = []\n",
    "for number , data in enumerate(conteudo_id):\n",
    "    contents[number] = data[number]['content']\n",
    "    annotations[number] = data[number]['annotation']   \n",
    "    for annotation in data[number]['annotation']:\n",
    "        #print(unidecode(annotation[0]['points'][0]['text'].lower().replace('\\'','').replace('\\\"','')) if annotation[0]['label'][0] != 'CUMPRIMENTO' else 'cumprimento')\n",
    "        keywords.append(re.sub(r'[.\"0-9]','',unidecode(annotation[0]['points'][0]['text'].lower().replace('\\'','').replace('\\\"',''))) if annotation[0]['label'][0] != 'CUMPRIMENTO' else 'cumprimento')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baixando o dataSet pra treino\n",
    "#nltk.corpus.mac_morpho.words()\n",
    "#nltk.corpus.mac_morpho.tagged_words()\n",
    "tagged = nltk.corpus.mac_morpho.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusList = []\n",
    "for new in tagged:\n",
    "    if new[0] != '':\n",
    "        CorpusList.append((unidecode(new[0]),new[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_tuple = []\n",
    "stop_words = stopwords.words('portuguese')\n",
    "for keyword in keywords:\n",
    "    if keyword != 'cumprimento':\n",
    "        if len(keyword.split()) > 1:\n",
    "            keyword_split = [y for y in keyword.split()]\n",
    "            for key in keyword_split:\n",
    "                if key not in stop_words:\n",
    "                    keywords_tuple.append((key,'K'))\n",
    "                else:\n",
    "                    keywords_tuple.append((key,'STOP'))\n",
    "        else:        \n",
    "            keywords_tuple.append((keyword,'K'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tirando do dataSet mac_morpho as palavras que existem no meu dataSet custom\n",
    "import time\n",
    "start = time.time()\n",
    "print(len(CorpusList))\n",
    "for i, key in enumerate(keywords_tuple):\n",
    "    for j, data in enumerate(CorpusList):\n",
    "        if data[0] == key[0]:\n",
    "            CorpusList.pop(j)\n",
    "    print(i+1,' de ',len(keywords_tuple))        \n",
    "print(len(CorpusList))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keywords_tuple:\n",
    "    CorpusList.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['-']\n",
    "for x in CorpusList:\n",
    "    if x[0] in lista:\n",
    "        print(x)\n",
    "\n",
    "for x in keywords_tuple:\n",
    "    if x[0] in lista:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.shuffle(CorpusList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusList.remove(('-','K'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist = []\n",
    "taglist.append(CorpusList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger(load=False) # don't load existing model\n",
    "tagger.train(taglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process(sent):\n",
    "    sent = nltk.word_tokenize(unidecode(sent.lower()))\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    sent = tagger.tag(sent)\n",
    "    new_sent = []\n",
    "    returner = []\n",
    "    aux_sent = ''\n",
    "    pattern = '''NP:  {<K><STOP>*<K>+} \n",
    "                      {<K>+}'''\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    cs = cp.parse(sent)\n",
    "    for i in cs:\n",
    "        if len(i) < 2 and i[0][1] == 'K':\n",
    "            returner.append(i[0][0])\n",
    "        elif len(i) > 2:\n",
    "            for j in i:\n",
    "                aux_sent += j[0]+' '\n",
    "            returner.append(aux_sent)\n",
    "            aux_sent = ''\n",
    "        elif isinstance(i[0],tuple):\n",
    "            returner.append(i[0][0]+' '+i[1][0])\n",
    "    return returner        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process(sent):\n",
    "    sent = nltk.word_tokenize(unidecode(sent.lower()))\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    sent = tagger.tag(sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = \"Bom dia. Na página 5 da aula 04, tem o fluxograma sobre a estrutura de repetição while, conforme anexo. Ao chegar na parte da condição (cont>10), o algoritmo já irá finalizar a lógica pois, iniciando o contador com zero (cont<-0) este sempre será menor que 10, finalizando o algoritmo. É esse o raciocínio? Atc.\"\n",
    "sent = process(text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salva um modelo\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(tagger, 'C:\\\\Users\\\\92007848\\\\Documents\\\\AIProj\\\\AiTeste\\\\NLTKAlpha\\\\Resources\\\\Models\\\\AlphaTagger3_003.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '''NP:  {<K><STOP>*<K>+} \n",
    "                  {<K>+}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cs:\n",
    "    print(type(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cs:\n",
    "    if len(i) < 2:\n",
    "        print(i)\n",
    "        print(i[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cs:\n",
    "    if (len(i) > 2 or (len(i) < 2 and i[0][1] == 'K')):\n",
    "        print(i[0][1])\n",
    "        for j in i:\n",
    "            print(j)\n",
    "        print('')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKProcessor:\n",
    "    def __init__(self, stop_words = None, pattern: None, tokenizer = lambda x: x.split(), ):\n",
    "        self.tagger = PerceptronTagger()\n",
    "        self.stop_words = stop_words\n",
    "        self.pattern = pattern\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def process(sent):\n",
    "        sent = self.tokenizer(unidecode(sent.lower()))\n",
    "        sent = self.tagger.tag(sent)\n",
    "        returner = []\n",
    "        aux_sent = ''\n",
    "        cp = nltk.RegexpParser(self.pattern)\n",
    "        cs = cp.parse(sent)\n",
    "        for i in cs:\n",
    "            if len(i) < 2 and i[0][1] == 'K':\n",
    "                returner.append(i[0][0])\n",
    "            elif len(i) > 2:\n",
    "                for j in i:\n",
    "                    aux_sent += j[0]+' '\n",
    "                returner.append(aux_sent)\n",
    "                aux_sent = ''\n",
    "            elif isinstance(i[0],tuple):\n",
    "                returner.append(i[0][0]+' '+i[1][0])\n",
    "        return returner  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
